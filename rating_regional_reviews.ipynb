{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22JfO-XGPXBV"
      },
      "source": [
        "### Imports and Downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcX4ydoU1tin",
        "outputId": "5b48c07b-2410-4290-dc4b-25dba60e1e4d"
      },
      "outputs": []
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind \n",
        "\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import bigrams\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glQ3d4vhPXBX"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pQPcOjHQ1IJp"
      },
      "outputs": [],
      "source": [
        "data_path = \"./final_project_data\"\n",
        "\n",
        "east_reviews = pd.read_json(f\"{data_path}/EAST_data_mini.json\", lines=True)\n",
        "midwest_reviews = pd.read_json(f\"{data_path}/MIDWEST_data_mini.json\", lines=True)\n",
        "south_reviews = pd.read_json(f\"{data_path}/SOUTH_data_mini.json\", lines=True)\n",
        "west_reviews = pd.read_json(f\"{data_path}/WEST_data_mini.json\", lines=True)\n",
        "\n",
        "all_reviews = pd.concat([east_reviews, midwest_reviews, south_reviews, west_reviews], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDe55AOPXBY"
      },
      "source": [
        "### Extracting Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICc4IiQy2Tdp"
      },
      "outputs": [],
      "source": [
        "def extract_all_text():\n",
        "    \"\"\" Concatenates all reviews into a single lowercase string \"\"\"\n",
        "    all_text = \"\"\n",
        "    for text in all_reviews['text']:\n",
        "        all_text += text + \" \"\n",
        "    return all_text.lower()\n",
        "\n",
        "def extract_vocab(all_text, size, collocational):\n",
        "    \"\"\" Returns a vector of the vocabulary to use for features\n",
        "        Input: \n",
        "          - all_text: str, result of extract_all_text()\n",
        "          - size: int, the length of the vocabulary\n",
        "          - collocational: boolean, use bigrams if true and BOW - stopwords otherwise\n",
        "    \"\"\"\n",
        "\n",
        "    words = word_tokenize(all_text)\n",
        "\n",
        "    if not collocational:\n",
        "        word_count = Counter(words)\n",
        "\n",
        "        # remove punctuation and stop words\n",
        "        for stopword in stopwords.words('english'):\n",
        "            del word_count[stopword]\n",
        "        for punc in punctuation:\n",
        "            del word_count[punc]\n",
        "\n",
        "        # sort by most common and return top {size} as vocab\n",
        "        vocab = np.asarray(list(zip(*word_count.most_common(size)))[0])\n",
        "    \n",
        "    else:\n",
        "        # return top {size} bigrams\n",
        "        bigram_count = Counter(bigrams(words))\n",
        "\n",
        "        bigram_list = list(list(zip(*bigram_count.most_common(size)))[0])\n",
        "        vocab = np.empty(len(bigram_list), dtype=object)\n",
        "        vocab[:] = bigram_list\n",
        "\n",
        "    return vocab\n",
        "\n",
        "def get_feature_vec(text, vocab, sentiment):\n",
        "    \"\"\" Returns the feature vector for a review\n",
        "        Input:\n",
        "          - text: str, the review\n",
        "          - vocab: array, result of extract_vocab()\n",
        "          - sentiment: boolean, concatenates VADER sentiment analysis result if true \n",
        "    \"\"\"\n",
        "    \n",
        "    words = word_tokenize(text)\n",
        "    vocab_vec = np.zeros(len(vocab))\n",
        "\n",
        "    if len(words) == 0:\n",
        "        return vocab_vec\n",
        "\n",
        "    # fill vocab vector\n",
        "    collocational = isinstance(vocab[0], tuple) \n",
        "    counts = Counter(bigrams(words)) if collocational else Counter(words)\n",
        "    for i in range(len(vocab)):\n",
        "        vocab_vec[i] = counts[vocab[i]]\n",
        "    \n",
        "    # normalize to percentages (so size of review doesn't matter)\n",
        "    total_count = np.sum(vocab_vec)\n",
        "    normalized_vec = vocab_vec/total_count if total_count > 0 else vocab_vec\n",
        "    \n",
        "    # add average sentiment scores from VADER\n",
        "    if sentiment:\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        avg_sentiment = {'compound': 0, 'neg': 0, 'neu': 0, 'pos': 0}\n",
        "        for sent in sentences:\n",
        "            for k,v in analyzer.polarity_scores(sent).items():\n",
        "                avg_sentiment[k] += v / len(sentences)\n",
        "\n",
        "        for k in ['compound', 'neg', 'neu', 'pos']:\n",
        "            normalized_vec = np.append(normalized_vec, avg_sentiment[k])\n",
        "\n",
        "    return normalized_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQBoLaIRPXBZ"
      },
      "source": [
        "### Split Data for Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af5j6YUsG3lO"
      },
      "outputs": [],
      "source": [
        "def get_X_and_Y(reviews, vocab, sentiment):\n",
        "    \"\"\" Returns the feature matrix X and the label vector Y for a set of reviews\n",
        "        Input:\n",
        "          - reviews: dataframe, a subset of all_reviews\n",
        "          - vocab: array, result of extract_vocab()\n",
        "          - sentiment: boolean, concatenates VADER sentiment analysis result if true \n",
        "    \"\"\"\n",
        "\n",
        "    Y = np.array(reviews['stars'])\n",
        "    X = np.empty((len(reviews), len(vocab) + 4*sentiment))\n",
        "    for i in range(len(reviews)):\n",
        "        X[i] = get_feature_vec(reviews['text'].iloc[i], vocab, sentiment)\n",
        "    return X, Y\n",
        "\n",
        "def get_region_split(region_to_test, vocab, sentiment):\n",
        "    \"\"\" Returns the training and testing feature matrices and labels for a given region split\n",
        "        Input:\n",
        "          - region_to_test: string, one of WEST, MIDWEST, SOUTH, EAST\n",
        "          - vocab: array, result of extract_vocab()\n",
        "          - sentiment: boolean, concatenates VADER sentiment analysis result if true \n",
        "    \"\"\"\n",
        "\n",
        "    train_reviews = all_reviews[all_reviews['region'] != region_to_test]\n",
        "    test_reviews = all_reviews[all_reviews['region'] == region_to_test]\n",
        "\n",
        "    train_X, train_Y = get_X_and_Y(train_reviews, vocab, sentiment)\n",
        "    test_X, test_Y = get_X_and_Y(test_reviews, vocab, sentiment)  \n",
        "\n",
        "    return train_X, train_Y, test_X, test_Y\n",
        "\n",
        "def get_random_split():\n",
        "    \"\"\" Randomly splits the reviews into four groups, must still call get_X_and_Y afterwards \"\"\"\n",
        "    split_size = len(all_reviews)//4\n",
        "\n",
        "    splits = []\n",
        "    remaining = all_reviews\n",
        "    for _ in range(4):\n",
        "        split = remaining.sample(split_size)\n",
        "        remaining = remaining.drop(split.index)\n",
        "        splits.append(split)\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVgjfxWjUiLw"
      },
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnDD2JiqVk6N"
      },
      "outputs": [],
      "source": [
        "def get_errors(model, train_X, train_Y, test_X, test_Y):\n",
        "    \"\"\" Trains a model and gets the errors from its predictions on a test set\n",
        "        Input:\n",
        "          - model: a sklearn style model with fit and predict methods\n",
        "          - train_X, train_Y, test_X, test_Y: train and test feature matrices (X) and labels (Y)\n",
        "    \"\"\"\n",
        "    model.fit(train_X, train_Y)\n",
        "    predictions = model.predict(test_X)\n",
        "    errors = []\n",
        "    for i in range(len(test_Y)):\n",
        "        # error is measured by star rating difference\n",
        "        errors.append(abs(test_Y[i]-predictions[i]))\n",
        "    return errors\n",
        "\n",
        "def print_significance(p):\n",
        "    \"\"\" Helper for evaluate_model(), prints whether the p-value is significant\"\"\"\n",
        "    if p < 0.01:\n",
        "        print(\"Significant at 99%\\n\")\n",
        "    elif p < 0.05:\n",
        "        print(\"Significant at 95%\\n\")\n",
        "    elif p < 0.1:\n",
        "        print(\"Significant at 90%\\n\")\n",
        "    else:\n",
        "        print(\"Not significant\\n\")\n",
        "\n",
        "def evaluate_model(model, collocational, sentiment, vocab_size):\n",
        "    \"\"\" Prints out the report for a model with given settings\n",
        "        Input:\n",
        "          - model: a sklearn style model with fit and predict methods\n",
        "          - collocational: boolean, use bigrams if true and BOW - stopwords otherwise\n",
        "          - sentiment: boolean, concatenates VADER sentiment analysis result to feature vector if true \n",
        "          - vocab_size: int, the length of the vocabulary\n",
        "    \"\"\"\n",
        "    text = extract_all_text()\n",
        "    vocab = extract_vocab(text, vocab_size, collocational)\n",
        "\n",
        "    # get errors for true cross validation\n",
        "    splits = get_random_split()\n",
        "    baseline_errors = []\n",
        "    for i in range(4):\n",
        "        test = splits[i]\n",
        "        test_X, test_Y = get_X_and_Y(test, vocab, sentiment)\n",
        "\n",
        "        train = pd.concat(splits[:i] + splits[i+1:], ignore_index=True)\n",
        "        train_X, train_Y = get_X_and_Y(train, vocab, sentiment)\n",
        "\n",
        "        errors = get_errors(model, train_X, train_Y, test_X, test_Y)\n",
        "        baseline_errors.extend(errors)\n",
        "    print(f\"\\nBASELINE ERROR: {np.mean(baseline_errors)}\\n\")\n",
        "\n",
        "    # get errors for regional splits\n",
        "    all_errors = []\n",
        "    for region in [\"WEST\", \"MIDWEST\", \"SOUTH\", \"EAST\"]:\n",
        "        train_X, train_Y, test_X, test_Y = get_region_split(region, vocab, sentiment)\n",
        "        errors = get_errors(model, train_X, train_Y, test_X, test_Y)\n",
        "        all_errors.extend(errors)\n",
        "        tstat, p = ttest_ind(errors, baseline_errors)\n",
        "        print(f\"{region} ERROR: {np.mean(errors)}\")\n",
        "        print(f\"Two-Sided T Test with BASELINE gives t-stat {tstat} and p-value {p}\")\n",
        "        print_significance(p)\n",
        "    \n",
        "    tstat, p = ttest_ind(all_errors, baseline_errors)\n",
        "    print(f\"ALL REGIONS ERROR: {np.mean(all_errors)}\")\n",
        "    print(f\"Two-Sided T Test with BASELINE gives t-stat {tstat} and p-value {p}\")\n",
        "    print_significance(p)\n",
        "\n",
        "def run_experiment(model):\n",
        "    \"\"\" Runs full set of tests for a given model \n",
        "        Input:\n",
        "          - model: a sklearn style model with fit and predict methods\n",
        "    \"\"\"\n",
        "    for collocational in [True, False]:\n",
        "        for sentiment in [False, True]:\n",
        "            print(f\"COLLOCATIONAL: {collocational}, SENTIMENT: {sentiment}\\n\")\n",
        "            evaluate_model(model, collocational, sentiment, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_livkRQcRffr"
      },
      "source": [
        "### Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJg8QwcuolKN"
      },
      "outputs": [],
      "source": [
        "print(\"GAUSSIAN\")\n",
        "run_experiment(GaussianNB())\n",
        "\n",
        "print(\"\\nXGBOOST\")\n",
        "run_experiment(XGBRegressor())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "rating_regional_reviews.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
